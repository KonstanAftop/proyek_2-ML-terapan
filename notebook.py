# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/KonstanAftop/proyek_2-ML-terapan/blob/main/notebook.ipynb

### Import Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
# Reproducibility
import random
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
tf.random.set_seed(SEED)

"""### Load Data"""

df_ratings = pd.read_csv('ml-100k/ml-latest-small/ratings.csv')
df_tags = pd.read_csv('ml-100k/ml-latest-small/tags.csv')
df_movies = pd.read_csv('ml-100k/ml-latest-small/movies.csv')

df_rating_tags = pd.merge(df_ratings, df_tags, on=['userId', 'movieId'], how='inner')
df_final = pd.merge(df_rating_tags, df_movies, on=['movieId'], how='inner')
df_final.drop(columns=['timestamp_x', 'timestamp_y'], inplace=True)

df_final.head(8)

"""### EDA"""

print(f'Banyak data : {df_final.shape[0]}')
print(f'Banyak atribut : {df_final.shape[1]}')

df_final.info()

df_final.isnull().sum()

df_final.duplicated().sum()

df_final['rating'].value_counts().sort_index().plot(kind='bar', title='Distribusi Rating')
plt.show()

from collections import Counter

genre_counts = Counter()
df_final['genres'].str.split('|').apply(genre_counts.update)
pd.Series(genre_counts).sort_values(ascending=False).plot(kind='bar', title='Frekuensi Genre')

"""### Data Preparation"""

df_final.head(10)

df_final_content_based = df_final.groupby('title').agg({
    'genres': 'first',  # atau pakai mode/first
    'tag': lambda x: ' '.join(set(x))  # gabung tag unik
}).reset_index()

df_final_content_based['genres_clean'] = df_final_content_based['genres'].str.replace('|', ' ', regex=False)

df_final_content_based['content_based_features'] = df_final_content_based['genres_clean'].str.lower() + ' ' + df_final_content_based['tag'].str.lower()

df_final_content_based

df_final_collaborative = df_final[['userId','movieId','rating','genres','tag']]

df_final_collaborative

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df_final_collaborative['userId'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah userID menjadi list tanpa nilai yang sama
movie_ids = df_final_collaborative['movieId'].unique().tolist()
print('list movieId: ', movie_ids)

# Melakukan encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
print('encoded movieId : ', movie_to_movie_encoded)

# Melakukan proses encoding angka ke ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}
print('encoded angka ke movieId: ', movie_encoded_to_movie)

# Mapping userID ke dataframe user
df_final_collaborative['user_encoded'] = df_final_collaborative['userId'].map(user_to_user_encoded)

df_final_collaborative['movie_encoded'] = df_final_collaborative['movieId'].map(movie_to_movie_encoded)

df_final_collaborative['rating']=df_final_collaborative['rating'].values.astype(np.float64)

df_final_collaborative = df_final_collaborative.sample(frac=1, random_state=42)
df_final_collaborative.head(5)

"""### Model Development Content Based Filtering"""

df_final_content_based.head(5)

tf = TfidfVectorizer(analyzer='word',min_df=0.0, stop_words='english')

tf.fit(df_final_content_based['content_based_features'])

tf.get_feature_names_out()

tfidf_matrix = tf.fit_transform(df_final_content_based['content_based_features'])

tfidf_matrix.shape

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim_df = pd.DataFrame(cosine_sim, index=df_final_content_based['title'], columns=df_final_content_based['title'])

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=df_final_content_based.title
).sample(50, axis=1).sample(10, axis=0)

cosine_sim_df.head(5)

def recommend_movies(title, cosine_sim_df=cosine_sim_df, top_n=5):
    if title not in cosine_sim_df.index:
        return f"Film '{title}' tidak ditemukan di dalam dataset."

    sim_scores = cosine_sim_df.loc[title]

    sim_scores = sim_scores.sort_values(ascending=False)
    return sim_scores.iloc[1:top_n+1]

cosine_sim_df.index = cosine_sim_df.index.astype(str)
cosine_sim_df.columns = cosine_sim_df.columns.astype(str)

result=recommend_movies("Sweet Charity (1969)", top_n=10)

def find_tag_genres(title):
    result=df_final_content_based[df_final_content_based.title==title]
    return (result[['title', 'content_based_features']])

find_tag_genres("Sweet Charity (1969)")

result_df = pd.DataFrame()
for p in result.index:
    df = find_tag_genres(p)
    result_df = pd.concat([result_df, df], ignore_index=True)

result_df

"""#### Evaluasi dengan Precision@10

Kita mengambil sampel untuk film "Sweet Charity (1969)" dengan genre+tag : comedy drama musical romance prostitution
Kita menentukan rekomendasi relevan dengan ketentuan :
Apabila terdapat minimal satu bagian dari genre atau tag yang sesuai, maka dianggap relevan.
Sehingga, utk K=10 rekomendasi, diperoleh ke 10 rekomendasi relevan.
"""

precision10 = 10/10
print(f'precision@10 = ', precision10)

"""### Model Development Collaborative Filtering"""

df_final_collaborative

min_rating = df_final_collaborative['rating'].min()
max_rating = df_final_collaborative['rating'].max()

x = df_final_collaborative[['user_encoded','movie_encoded']].values
y = df_final_collaborative['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * x.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_movies = len(movie_encoded_to_movie)
print(num_movies)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
tf.random.set_seed(SEED)
class RecommenderNet(tf.keras.Model):

  # Inisialisasi model
  def __init__(self, num_users, num_movies, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)

    self.num_users = num_users
    self.num_movies = num_movies
    self.embedding_size = embedding_size

    # Layer embedding untuk user
    self.user_embedding = layers.Embedding(
        input_dim=num_users,
        output_dim=embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-6)
    )

    # Layer bias untuk user
    self.user_bias = layers.Embedding(
        input_dim=num_users,
        output_dim=1
    )

    # Layer embedding untuk film
    self.movies_embedding = layers.Embedding(
        input_dim=num_movies,
        output_dim=embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-6)
    )

    # Layer bias untuk film
    self.movies_bias = layers.Embedding(
        input_dim=num_movies,
        output_dim=1
    )

  # Proses forward (prediksi)
  def call(self, inputs):
    # Ambil vektor embedding dan bias berdasarkan user dan film
    user_vector = self.user_embedding(inputs[:, 0])       # vektor user
    user_bias = self.user_bias(inputs[:, 0])              # bias user
    movies_vector = self.movies_embedding(inputs[:, 1])   # vektor film
    movies_bias = self.movies_bias(inputs[:, 1])          # bias film

    # Hitung kecocokan (dot product) antara user dan film
    dot_user_movies = tf.tensordot(user_vector, movies_vector, axes=2)

    # Tambahkan bias
    x = dot_user_movies + user_bias + movies_bias

    # Aktivasi sigmoid untuk mengubah output ke rentang [0, 1]
    return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_movies, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 30,
    validation_data = (x_val, y_val)
)

plt.plot(history.history['root_mean_squared_error'], label='training')
plt.plot(history.history['val_root_mean_squared_error'], label='validation')
plt.legend()
plt.title('RMSE Training vs Validation')
plt.xlabel('Epoch')
plt.ylabel(('RMSE'))
plt.grid(True)
plt.show()

df_final

user_id = df_final.userId.sample(1).iloc[0]

movies_watched = df_final[df_final.userId == user_id]

movies_not_watched = df_final[~df_final['movieId'].isin(movies_watched.movieId.values)]['movieId']

movies_not_watched = list(
    set(movies_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movies_not_watched = [[movie_to_movie_encoded.get(x)] for x in movies_not_watched]

user_encoder = user_to_user_encoded.get(user_id)

user_movie_array = np.hstack(
    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)
)

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]

top_ratings_indices

recommended_movies_ids = [
    movie_encoded_to_movie.get(movies_not_watched[x][0]) for x in top_ratings_indices
]

recommended_movies_ids

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movies with high ratings from user')
print('----' * 8)

top_movie_user = (
    movies_watched.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.unique() # Tambahkan .unique() di sini
)
merged_df = movies_watched.groupby(['userId', 'movieId', 'title', 'genres', 'rating'])['tag'].apply(lambda x: ', '.join(x)).reset_index()
movie_df_rows = merged_df[merged_df['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genres, row.rating)

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

df_recommend = df_final.groupby(['userId', 'movieId', 'title', 'genres', 'rating'])['tag'].apply(lambda x: ', '.join(x)).reset_index()
recommended_movie = df_recommend[df_recommend['movieId'].isin(recommended_movies_ids)].drop_duplicates(subset=['title'])
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)

"""#### Evaluasi dengan Metrik RMSE"""

y_true = y_val
y_pred = model.predict(x_val)

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print("RMSE:", rmse)