# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/KonstanAftop/proyek_2-ML-terapan/blob/main/notebook.ipynb
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
# Reproducibility
import random
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
tf.random.set_seed(SEED)

"""# Data Understanding"""

df_ratings = pd.read_csv('dataset/ml-latest-small/ratings.csv')
df_tags = pd.read_csv('dataset/ml-latest-small/tags.csv')
df_movies = pd.read_csv('dataset/ml-latest-small/movies.csv')

df_ratings.head(3)

"""Pada data ratings, terdapat 4 variabel :
- userId : nomor unik user
- movieId : nomor unik film
- rating : rating yang diberikan user untuk film
- timestamp :  merepresentasikan jumlah detik sejak tengah malam Waktu Universal Terkoordinasi (UTC) pada tanggal 1 Januari 1970.
"""

df_movies.head(3)

"""Pada data movies, terdapat 3 variabel :
- movieId : nomor unik film
- title : judul film dengan nomor unik movieId
- genres : genre dari film
"""

df_tags.head(3)

"""Pada data tags, terdapat 4 variabel :
- userId : nomor unik user
- movieId : nomor unik film
- tag : tag yang diberikan user untuk film
- timestamp : merepresentasikan jumlah detik sejak tengah malam Waktu Universal Terkoordinasi (UTC) pada tanggal 1 Januari 1970.
"""

# Banyak Data
print(f'Banyak data ratings film yang dimiliki : {len(df_ratings)}')
print(f'Banyak data film film yang dimiliki : {len(df_movies)}')
print(f'Banyak data tags film yang dimiliki : {len(df_tags)}')

"""# Exploratory Data Analysis"""

df_tags.info()

df_tags.isnull().sum()

df_tags[df_tags.duplicated(subset=['userId','movieId'], keep=False)]

"""Pada data tags, terdapat tag yang terpisah untuk userId dan movieId yang sama."""

df_movies.info()

df_movies.isnull().sum()

df_movies[df_movies.duplicated(subset=['title'], keep=False)]

"""Pada data movies, terdapat film yang sama ditandai dengan nomor unik yang berbeda."""

print(f'Banyaknya film yang tersedia : {df_movies.title.nunique()}')

df_ratings.info()

df_ratings.isnull().sum()

df_ratings[df_ratings.duplicated(subset=['userId', 'movieId'], keep=False)]

"""Tidak terdapat data missing pada ketiga dataframe. Namun terdapat beberapa nilai yang duplikat."""

plt.hist(df_ratings['rating'],bins=10)
plt.title('Distribusi Rating')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.show()

all_genres = df_movies['genres'].str.split('|').explode()
genre_counts = all_genres.value_counts().head(10)

plt.figure(figsize=(10, 6))
genre_counts.plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Top 10 Genre Terbanyak dalam Dataset')
plt.xlabel('Genre')
plt.ylabel('Jumlah Film')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# Data Preparation"""

# Membuang fitur yang tidak relevan/tidak digunakan
df_ratings = df_ratings.drop(columns='timestamp')
df_tags = df_tags.drop(columns='timestamp')

"""Berdasarkan permasalahan yang kita temukan di EDA, terdapat duplikasi tag pada userId dan movieId yang sama. Maka, kita perlu menggabungkannya."""

df_tags_cleaned = df_tags.groupby(['movieId','userId']).agg({
    'tag' : lambda x: ' '.join(set(x))
}).reset_index()

df_tags_cleaned.head(5)

"""Kemudian, kita atasi masalah movie dengan nomor unik yang berbeda."""

df_movies_cleaned=df_movies.drop_duplicates(subset='title', keep='first')

df_movies_cleaned.head(5)

df_rating_tags = pd.merge(df_ratings, df_tags_cleaned, on=['userId', 'movieId'], how='inner')
df_final = pd.merge(df_rating_tags, df_movies_cleaned, on=['movieId'], how='inner')

df_final2 = pd.merge(df_ratings, df_movies_cleaned, on='movieId', how='inner')

"""- df_final : digunakan untuk content based (dengan tags)
- df_final2 : digunakan untuk collaborative filtering (tanpa tags)

## Data Preparation for Content-Based Filtering
"""

df_final_content_based = df_final.copy()

df_final_content_based['genres'] = df_final_content_based['genres'].str.replace('|', ' ', regex=False)

df_final_content_based['extracted_features'] = df_final_content_based['genres'].str.lower() + ' ' + df_final_content_based['tag'].str.lower()

df_final_content_based

tf = TfidfVectorizer(analyzer='word',min_df=0.0, stop_words='english')

tf.fit(df_final_content_based['extracted_features'])

tf.get_feature_names_out()

content_based_tfidf_matrix = tf.fit_transform(df_final_content_based['extracted_features'])

"""## Data Preparation for Collaborative Filtering Model Based Deep Learning"""

df_final_collaborative = df_final2.copy()

df_final_collaborative

# Mengubah userId menjadi list tanpa nilai yang sama
user_ids = df_final_collaborative['userId'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = df_final_collaborative['movieId'].unique().tolist()
print('list movieId: ', movie_ids)

# Melakukan encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
print('encoded movieId : ', movie_to_movie_encoded)

# Melakukan proses encoding angka ke ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}
print('encoded angka ke movieId: ', movie_encoded_to_movie)

df_final_collaborative['user_encoded'] = df_final_collaborative['userId'].map(user_to_user_encoded)
df_final_collaborative['movie_encoded'] = df_final_collaborative['movieId'].map(movie_to_movie_encoded)
df_final_collaborative['rating']=df_final_collaborative['rating'].values.astype(np.float64)

# Shuffling data sebelum displit
df_final_collaborative = df_final_collaborative.sample(frac=1, random_state=42)
df_final_collaborative.head(5)

min_rating = df_final_collaborative['rating'].min()
max_rating = df_final_collaborative['rating'].max()

x = df_final_collaborative[['user_encoded','movie_encoded']].values
y = df_final_collaborative['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * x.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

x,y

"""# Model Development Content Based Filtering

Melakukan perhitungan cosine similiarity antar Film
"""

cosine_sim = cosine_similarity(content_based_tfidf_matrix)
cosine_sim_df = pd.DataFrame(cosine_sim, index=df_final_content_based['title'], columns=df_final_content_based['title'])

cosine_sim_df.head(5)

def recommend_movies(title, cosine_sim_df=cosine_sim_df, top_n=5):
    #Fungsi untuk mendapatkan rekomendasi top n dengan similiarity score tertinggi
    if title not in cosine_sim_df.index:
        return f"Film '{title}' tidak ditemukan di dalam dataset."

    sim_scores = cosine_sim_df.loc[title]

    sim_scores = sim_scores.sort_values(ascending=False)
    return sim_scores.iloc[1:top_n+1]

cosine_sim_df.index = cosine_sim_df.index.astype(str)
cosine_sim_df.columns = cosine_sim_df.columns.astype(str)

# Mendapatkan rekomendasi film lain saat menyukai film 'Sweet Charity (1969)'
recommendation = recommend_movies("Sweet Charity (1969)", top_n=10)

recommendation

df_final_content_based[df_final_content_based.title=='Sweet Charity (1969)'][['title', 'extracted_features']]

def find_tag_genres(title):
    result=df_final_content_based[df_final_content_based.title==title].drop_duplicates(subset='title', keep='first')
    return (result[['title', 'extracted_features']])

"""Memberikan rekomendasi dalam bentuk dataframe"""

recommendation_df = pd.DataFrame()
for p in recommendation.index:
    df = find_tag_genres(p)
    recommendation_df = pd.concat([recommendation_df, df], ignore_index=True)

recommendation_df

"""### Evaluasi dengan Precision@10

Kita mengambil sampel untuk film "Sweet Charity (1969)" dengan genre+tag : comedy drama musical romance prostitution
Kita menentukan rekomendasi relevan dengan ketentuan :
Apabila terdapat minimal satu bagian dari genre atau tag yang sesuai, maka dianggap relevan.
Sehingga, utk K=10 rekomendasi, diperoleh 10 rekomendasi relevan.
"""

precision10 = 10/10
print(f'precision@10 = ', precision10)

"""Diperoleh precision@10, artinya dari 10 rekomendasi yang diberikan, terdapat 10 rekomendasi yang relevan/sesuai.

# Model Development Collaborative Filtering
"""

num_users = len(user_to_user_encoded)
print(num_users)

num_movies = len(movie_encoded_to_movie)
print(num_movies)

"""Pembangunan Arsitektur Model"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
tf.random.set_seed(SEED)
class RecommenderNet(tf.keras.Model):

  # Inisialisasi model
  def __init__(self, num_users, num_movies, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)

    self.num_users = num_users
    self.num_movies = num_movies
    self.embedding_size = embedding_size

    # Layer embedding untuk user
    self.user_embedding = layers.Embedding(
        input_dim=num_users,
        output_dim=embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-4)
    )

    # Layer bias untuk user
    self.user_bias = layers.Embedding(
        input_dim=num_users,
        output_dim=1
    )

    # Layer embedding untuk film
    self.movies_embedding = layers.Embedding(
        input_dim=num_movies,
        output_dim=embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-4)
    )

    # Layer bias untuk film
    self.movies_bias = layers.Embedding(
        input_dim=num_movies,
        output_dim=1
    )

  # Proses forward (prediksi)
  def call(self, inputs):
    # Ambil vektor embedding dan bias berdasarkan user dan film
    user_vector = self.user_embedding(inputs[:, 0])       # vektor user
    user_bias = self.user_bias(inputs[:, 0])              # bias user
    movies_vector = self.movies_embedding(inputs[:, 1])   # vektor film
    movies_bias = self.movies_bias(inputs[:, 1])          # bias film

    # Hitung kecocokan (dot product) antara user dan film
    dot_user_movies = tf.tensordot(user_vector, movies_vector, axes=2)

    # Tambahkan bias
    x = dot_user_movies + user_bias + movies_bias

    # Aktivasi sigmoid untuk mengubah output ke rentang [0, 1]
    return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_movies, 30) # inisialisasi model

# model compile
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_root_mean_squared_error',
        patience=10,
        restore_best_weights=True
    )
] # callbacks untuk menghentikan training saat tidak ada perubahan signifikan terhadap validation rmse

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=100,    # Set ke angka yang cukup besar, early stopping akan menghentikan
    verbose=1,
    validation_data=(x_val, y_val),
    callbacks=callbacks
)

# Mendapatkan rekomendasi untuk user dengan nomor unik 483
user_id = df_final_collaborative.userId.sample(1, random_state=42).iloc[0]

# Mengambil film yang belum ditonton oleh user 483
movies_watched = df_final2[df_final2.userId == user_id]
movies_not_watched = df_final2[~df_final2['movieId'].isin(movies_watched.movieId.values)]['movieId']
movies_not_watched = list(
    set(movies_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movies_not_watched = [[movie_to_movie_encoded.get(x)] for x in movies_not_watched]
user_encoder = user_to_user_encoded.get(user_id)

user_movie_array = np.hstack(
    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)
)

ratings = model.predict(user_movie_array).flatten() #melakukan prediksi rating yang dinormalisasi

top_ratings_indices = ratings.argsort()[-10:][::-1] # mengambil film dengan nilai prediksi rating tertinggi

top_ratings_indices

recommended_movies_ids = [
    movie_encoded_to_movie.get(movies_not_watched[x][0]) for x in top_ratings_indices
]

recommended_movies_ids

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movies with high ratings from user')
print('----' * 8)

top_movie_user = (
    movies_watched.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.unique()
)
movie_df_rows = movies_watched[movies_watched['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genres, row.rating)

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movie = df_final_collaborative[df_final_collaborative['movieId'].isin(recommended_movies_ids)].drop_duplicates(subset=['title'])
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)

"""### Evaluation dengan RMSE"""

plt.plot(history.history['root_mean_squared_error'], label='training')
plt.plot(history.history['val_root_mean_squared_error'], label='validation')
plt.legend()
plt.title('RMSE Training vs Validation')
plt.xlabel('Epoch')
plt.ylabel(('RMSE'))
plt.grid(True)
plt.show()

y_true = y_val
y_pred = model.predict(x_val)

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print("RMSE:", rmse)

"""Berdasarkan grafik RMSE, terlihat bahwa ada indikasi overfitting yang bisa jadi disebabkan oleh kurang banyaknya data yang diperoleh.
Nilai RMSE sangat baik yaitu sekitar 0.1936, untuk skala 0-1 rating yang dinormalisasi.
"""